{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7eadf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Uniform\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR, OneCycleLR\n",
    "from torch_poly_lr_decay import PolynomialLRDecay\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import wandb\n",
    "\n",
    "PATH_DATASETS = \".\"\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "# BATCH_SIZE=1\n",
    "\n",
    "plt.rc('font', size=20)        # 기본 폰트 크기\n",
    "plt.rc('axes', labelsize=20)   # x,y축 label 폰트 크기\n",
    "plt.rc('xtick', labelsize=20)  # x축 눈금 폰트 크기 \n",
    "plt.rc('ytick', labelsize=20)  # y축 눈금 폰트 크기\n",
    "plt.rc('legend', fontsize=20)  # 범례 폰트 크기\n",
    "plt.rc('figure', titlesize=20) # figure title 폰트 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b334c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 8407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8407"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(8407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6347afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyData(Dataset):\n",
    "    def __init__(self, p_A, p_a, p_B, p_b, q_C, q_c, y):\n",
    "        self.X = torch.column_stack([p_A, p_a, p_B, p_b])\n",
    "        self.y = y\n",
    "        self.q_C = q_C\n",
    "        self.q_c = q_c\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.q_C[idx], self.q_c[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed979e9a",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b62679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.load('./data/toy_array_new.npz')\n",
    "data_2 = np.load('./data/1200900700_array.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd06787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_mass_sq(p):\n",
    "    return p[:,0]**2 - p[:,1]**2 - p[:,2]**2 - p[:,3]**2\n",
    "\n",
    "g = np.array([1,-1,-1,-1])\n",
    "def Mass(p, ax=1):\n",
    "    return torch.sqrt(p[:,0]**2 - p[:,1]**2 - p[:,2]**2 - p[:,3]**2)\n",
    "\n",
    "def npMass(p, ax=1):\n",
    "    return np.sqrt(p[:,0]**2 - p[:,1]**2 - p[:,2]**2 - p[:,3]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd0e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = 1000\n",
    "def pre_process(data_1, data_2, scaler):\n",
    "    N = min(data_1['b1'].shape[0], data_2['b1'].shape[0])\n",
    "    rand_idx_1 = np.random.randint(data_1['b1'].shape[0], size=N)\n",
    "    rand_idx_2 = np.random.randint(data_2['b1'].shape[0], size=N)\n",
    "    \n",
    "    pa11 = data_1['b1'][rand_idx_1,:] / scaler\n",
    "    pa12 = data_1['b2'][rand_idx_1,:] / scaler\n",
    "    pb11 = data_1['l2'][rand_idx_1,:] / scaler\n",
    "    pb12 = data_1['l1'][rand_idx_1,:] / scaler\n",
    "    qc11 = data_1['nu1'][rand_idx_1,:] / scaler\n",
    "    qc12 = data_1['nu2'][rand_idx_1,:] / scaler\n",
    "    y1 = np.repeat(0, pa11.shape[0]).reshape((-1, 1))\n",
    "    \n",
    "    pa21 = data_2['b1'][rand_idx_2,:] / scaler\n",
    "    pa22 = data_2['b2'][rand_idx_2,:] / scaler\n",
    "    pb21 = data_2['l2'][rand_idx_2,:] / scaler\n",
    "    pb22 = data_2['l1'][rand_idx_2,:] / scaler\n",
    "    qc21 = data_2['nu1'][rand_idx_2,:] / scaler\n",
    "    qc22 = data_2['nu2'][rand_idx_2,:] / scaler\n",
    "    y2 = np.repeat(1, pa21.shape[0]).reshape((-1, 1))\n",
    "    \n",
    "    np_momenta_1 = [pa11, pa12, pb11, pb12, qc11, qc12, y1]\n",
    "    np_momenta_2 = [pa21, pa22, pb21, pb22, qc21, qc22, y2]\n",
    "    \n",
    "    X = list(map(lambda x: np.row_stack((x[0],x[1])), zip(np_momenta_1, np_momenta_2)))\n",
    "    \n",
    "    torch_momenta = list(map(lambda x: torch.tensor(x, dtype=torch.float32), X))\n",
    "    \n",
    "    ds = ToyData(*torch_momenta)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b387e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pre_process(data_1, data_2, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcc971ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45c09eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.2411, -0.0433, -0.1255,  0.2012,  0.2963, -0.0293,  0.1904,  0.2251,\n",
       "          0.2542,  0.1916, -0.0917,  0.1398,  0.0466, -0.0209,  0.0414,  0.0045]),\n",
       " tensor([ 1.3007,  0.6305, -0.1054,  0.8906]),\n",
       " tensor([ 2.2242, -0.7285,  0.0907,  1.9794]),\n",
       " tensor([0.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce865d",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eb677ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyNet(pl.LightningModule):\n",
    "    def __init__(self, hparams=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        hidden_layer       = hparams[\"hidden_layer\"]\n",
    "        hidden_depth       = hparams[\"hidden_depth\"]\n",
    "        learning_rate      = hparams[\"learning_rate\"]\n",
    "        batch_size         = hparams[\"batch_size\"]\n",
    "        \n",
    "        self.hidden_layer  = hidden_layer\n",
    "        self.hidden_depth  = hidden_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size    = batch_size\n",
    "        self.epochs        = hparams[\"epochs\"]\n",
    "        self.gamma        = hparams[\"gamma\"]\n",
    "        self.max_lr        = hparams['max_lr']\n",
    "        self.learn_mode    = hparams['learn_mode'] # for pT mC loss on off\n",
    "        self.learn_mode_sq = hparams['learn_mode_sq'] # sq or sqrt\n",
    "        \n",
    "        m1_C = torch.tensor(hparams[\"m1_C_init\"])\n",
    "        m1_B = m1_C + torch.tensor(hparams[\"m1_B_add\"])\n",
    "        m1_A = m1_B + torch.tensor(hparams[\"m1_A_add\"])\n",
    "        \n",
    "        m2_C = torch.tensor(hparams[\"m2_C_init\"])\n",
    "        m2_B = m2_C + torch.tensor(hparams[\"m2_B_add\"])\n",
    "        m2_A = m2_B + torch.tensor(hparams[\"m2_A_add\"])\n",
    "        \n",
    "        if self.learn_mode_sq == 'sq':\n",
    "            m1_C = m1_C ** 2\n",
    "            m1_B = m1_B ** 2\n",
    "            m1_A = m1_A ** 2\n",
    "            m2_C = m2_C ** 2\n",
    "            m2_B = m2_B ** 2\n",
    "            m2_A = m2_A ** 2\n",
    "        \n",
    "        self.m1_C = m1_C\n",
    "        self.m1_B = nn.Parameter(m1_B, requires_grad=True)\n",
    "        self.m1_A = nn.Parameter(m1_A, requires_grad=True)\n",
    "        self.m2_C = m2_C\n",
    "        self.m2_B = nn.Parameter(m2_B, requires_grad=True)\n",
    "        self.m2_A = nn.Parameter(m2_A, requires_grad=True)\n",
    "        \n",
    "        layers = [nn.Linear(16, hidden_layer), nn.ReLU(inplace=True), nn.BatchNorm1d(hidden_layer)]\n",
    "        for i in range(hidden_depth):\n",
    "            layers.extend([\n",
    "                nn.Linear(hidden_layer, hidden_layer),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(hidden_layer)\n",
    "            ])\n",
    "            \n",
    "        if self.learn_mode == 'pt_mc':\n",
    "            layers.append(nn.Linear(hidden_layer, 8))\n",
    "        elif self.learn_mode in ['pt', 'mc']:\n",
    "            layers.append(nn.Linear(hidden_layer, 6))\n",
    "        elif self.learn_mode == None:\n",
    "            layers.append(nn.Linear(hidden_layer, 4))\n",
    "                \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        self.save_hyperparameters(hparams)\n",
    "        \n",
    "        self.ds = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "            \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _, _, y = batch\n",
    "        pa1 = x[:,0:4]\n",
    "        pa2 = x[:,4:8]\n",
    "        pb1 = x[:,8:12]\n",
    "        pb2 = x[:,12:16]\n",
    "        \n",
    "        y_0_ics = y == 0\n",
    "        y_1_ics = y == 1\n",
    "        \n",
    "        m_C = torch.zeros_like(y)\n",
    "        m_C[y_0_ics] = self.m1_C\n",
    "        m_C[y_1_ics] = self.m2_C\n",
    "        \n",
    "        q = self(x)\n",
    "        if self.learn_mode == 'pt_mc':\n",
    "            qc1 = q[:,0:4]\n",
    "            qc2 = q[:,4:8]\n",
    "        elif self.learn_mode == 'pt':\n",
    "            qx1 = q[:,0:1] \n",
    "            qy1 = q[:,1:2]\n",
    "            qx2 = q[:,2:3] \n",
    "            qy2 = q[:,3:4]\n",
    "            qz1 = q[:,4:5]   \n",
    "            qz2 = q[:,5:6]          \n",
    "            \n",
    "            Eq1 = torch.sqrt(m_C**2 + qx1**2 + qy1**2 + qz1**2)\n",
    "            Eq2 = torch.sqrt(m_C**2 + qx2**2 + qy2**2 + qz2**2)    \n",
    "            \n",
    "            qc1  = torch.cat([Eq1,qx1,qy1,qz1], 1)\n",
    "            qc2  = torch.cat([Eq2,qx2,qy2,qz2], 1) \n",
    "            \n",
    "        elif self.learn_mode == 'mc':\n",
    "            qx1 = q[:,0:1] \n",
    "            qy2 = q[:,1:2]\n",
    "            qz1 = q[:,2:3]   \n",
    "            qz2 = q[:,3:4]\n",
    "            Eq1 = q[:,4:5]\n",
    "            Eq2 = q[:,5:6]            \n",
    "\n",
    "            pTx = x[:,1:2]+x[:,5:6]+x[:,9:10]+x[:,13:14]\n",
    "            pTy = x[:,2:3]+x[:,6:7]+x[:,10:11]+x[:,14:15]\n",
    "\n",
    "            qx2 = -pTx-qx1\n",
    "            qy1 = -pTy-qy2\n",
    "\n",
    "            qc1  = torch.cat([Eq1,qx1,qy1,qz1], 1)\n",
    "            qc2  = torch.cat([Eq2,qx2,qy2,qz2], 1)    \n",
    "            \n",
    "        elif self.learn_mode == None:\n",
    "            qx1 = q[:,0:1] \n",
    "            qy2 = q[:,1:2]\n",
    "            qz1 = q[:,2:3]   \n",
    "            qz2 = q[:,3:4]\n",
    "\n",
    "            pTx = x[:,1:2]+x[:,5:6]+x[:,9:10]+x[:,13:14]\n",
    "            pTy = x[:,2:3]+x[:,6:7]+x[:,10:11]+x[:,14:15]\n",
    "\n",
    "            qx2 = -pTx-qx1\n",
    "            qy1 = -pTy-qy2\n",
    "\n",
    "            Eq1 = torch.sqrt(m_C**2 + qx1**2 + qy1**2 + qz1**2)\n",
    "            Eq2 = torch.sqrt(m_C**2 + qx2**2 + qy2**2 + qz2**2)\n",
    "\n",
    "            qc1  = torch.cat([Eq1,qx1,qy1,qz1], 1)\n",
    "            qc2  = torch.cat([Eq2,qx2,qy2,qz2], 1)        \n",
    "        \n",
    "        pB1 = pb1 + qc1\n",
    "        pB2 = pb2 + qc2\n",
    "        pA1 = pa1 + pB1\n",
    "        pA2 = pa2 + pB2\n",
    "        pT = (pA1 + pA2)[:,1:3]\n",
    "\n",
    "        if self.learn_mode_sq == 'sq':\n",
    "            mC1_sq = np_mass_sq(qc1)\n",
    "            mC2_sq = np_mass_sq(qc2)\n",
    "            mB1_sq = np_mass_sq(pB1)\n",
    "            mB2_sq = np_mass_sq(pB2)\n",
    "            mA1_sq = np_mass_sq(pA1)\n",
    "            mA2_sq = np_mass_sq(pA2)\n",
    "\n",
    "        elif self.learn_mode_sq == 'sqrt':\n",
    "            mC1_sq = Mass(qc1)\n",
    "            mC2_sq = Mass(qc2)\n",
    "            mB1_sq = Mass(pB1)\n",
    "            mB2_sq = Mass(pB2)\n",
    "            mA1_sq = Mass(pA1)\n",
    "            mA2_sq = Mass(pA2)\n",
    "        \n",
    "        mCs = torch.ones_like(mC1_sq)\n",
    "        mBs = torch.ones_like(mB1_sq)\n",
    "        mAs = torch.ones_like(mA1_sq)\n",
    "        mCs[y_0_ics[:,0]] *= self.m1_C\n",
    "        mBs[y_0_ics[:,0]] *= self.m1_B\n",
    "        mAs[y_0_ics[:,0]] *= self.m1_A\n",
    "        mCs[y_1_ics[:,0]] *= self.m2_C\n",
    "        mBs[y_1_ics[:,0]] *= self.m2_B\n",
    "        mAs[y_1_ics[:,0]] *= self.m2_A\n",
    "\n",
    "        loss_C = torch.abs(mC1_sq - mC2_sq) + torch.abs(mC1_sq - mCs) + torch.abs(mC2_sq - mCs)\n",
    "        loss_B = torch.abs(mB1_sq - mB2_sq) + torch.abs(mB1_sq - mBs) + torch.abs(mB2_sq - mBs)\n",
    "        loss_A = torch.abs(mA1_sq - mA2_sq) + torch.abs(mA1_sq - mAs) + torch.abs(mA2_sq - mAs)\n",
    "        \n",
    "        loss_pT = pT[:,0]**2 + pT[:,1]**2            \n",
    "        \n",
    "        loss_C = loss_C \n",
    "        loss_B = loss_B \n",
    "        loss_A = loss_A \n",
    "        \n",
    "        if self.learn_mode == 'pt_mc':\n",
    "            loss = (loss_A + loss_B + loss_C).mean() + loss_pT.mean()\n",
    "        elif self.learn_mode == 'pt':\n",
    "            loss = (loss_A + loss_B).mean() + loss_pT.mean()\n",
    "        elif self.learn_mode == 'mc':\n",
    "            loss = (loss_A + loss_B + loss_C).mean() \n",
    "        elif self.learn_mode == None:        \n",
    "            loss = (loss_A + loss_B).mean() \n",
    "                \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _, _, y = batch\n",
    "        pa1 = x[:,0:4]\n",
    "        pa2 = x[:,4:8]\n",
    "        pb1 = x[:,8:12]\n",
    "        pb2 = x[:,12:16]\n",
    "        \n",
    "        y_0_ics = y == 0\n",
    "        y_1_ics = y == 1\n",
    "        \n",
    "        m_C = torch.ones_like(y)\n",
    "        m_C[y_0_ics] *= self.m1_C\n",
    "        m_C[y_1_ics] *= self.m2_C\n",
    "        \n",
    "        q = self(x)\n",
    "        if self.learn_mode == 'pt_mc':\n",
    "            qc1 = q[:,0:4]\n",
    "            qc2 = q[:,4:8]\n",
    "        elif self.learn_mode == 'pt':\n",
    "            qx1 = q[:,0:1] \n",
    "            qy1 = q[:,1:2]\n",
    "            qx2 = q[:,2:3] \n",
    "            qy2 = q[:,3:4]\n",
    "            qz1 = q[:,4:5]   \n",
    "            qz2 = q[:,5:6]          \n",
    "            \n",
    "            Eq1 = torch.sqrt(m_C**2 + qx1**2 + qy1**2 + qz1**2)\n",
    "            Eq2 = torch.sqrt(m_C**2 + qx2**2 + qy2**2 + qz2**2)    \n",
    "            \n",
    "            qc1  = torch.cat([Eq1,qx1,qy1,qz1], 1)\n",
    "            qc2  = torch.cat([Eq2,qx2,qy2,qz2], 1) \n",
    "            \n",
    "        elif self.learn_mode == 'mc':\n",
    "            qx1 = q[:,0:1] \n",
    "            qy2 = q[:,1:2]\n",
    "            qz1 = q[:,2:3]   \n",
    "            qz2 = q[:,3:4]\n",
    "            Eq1 = q[:,4:5]\n",
    "            Eq2 = q[:,5:6]            \n",
    "\n",
    "            pTx = x[:,1:2]+x[:,5:6]+x[:,9:10]+x[:,13:14]\n",
    "            pTy = x[:,2:3]+x[:,6:7]+x[:,10:11]+x[:,14:15]\n",
    "\n",
    "            qx2 = -pTx-qx1\n",
    "            qy1 = -pTy-qy2\n",
    "\n",
    "            qc1  = torch.cat([Eq1,qx1,qy1,qz1], 1)\n",
    "            qc2  = torch.cat([Eq2,qx2,qy2,qz2], 1)    \n",
    "            \n",
    "        elif self.learn_mode == None:\n",
    "            qx1 = q[:,0:1] \n",
    "            qy2 = q[:,1:2]\n",
    "            qz1 = q[:,2:3]   \n",
    "            qz2 = q[:,3:4]\n",
    "\n",
    "            pTx = x[:,1:2]+x[:,5:6]+x[:,9:10]+x[:,13:14]\n",
    "            pTy = x[:,2:3]+x[:,6:7]+x[:,10:11]+x[:,14:15]\n",
    "\n",
    "            qx2 = -pTx-qx1\n",
    "            qy1 = -pTy-qy2\n",
    "\n",
    "            Eq1 = torch.sqrt(m_C**2 + qx1**2 + qy1**2 + qz1**2)\n",
    "            Eq2 = torch.sqrt(m_C**2 + qx2**2 + qy2**2 + qz2**2)\n",
    "\n",
    "            qc1  = torch.cat([Eq1,qx1,qy1,qz1], 1)\n",
    "            qc2  = torch.cat([Eq2,qx2,qy2,qz2], 1)        \n",
    "        \n",
    "        pB1 = pb1 + qc1\n",
    "        pB2 = pb2 + qc2\n",
    "        pA1 = pa1 + pB1\n",
    "        pA2 = pa2 + pB2\n",
    "        pT = (pA1 + pA2)[:,1:3]\n",
    "\n",
    "        if self.learn_mode_sq == 'sq':\n",
    "            mC1_sq = np_mass_sq(qc1)\n",
    "            mC2_sq = np_mass_sq(qc2)\n",
    "            mB1_sq = np_mass_sq(pB1)\n",
    "            mB2_sq = np_mass_sq(pB2)\n",
    "            mA1_sq = np_mass_sq(pA1)\n",
    "            mA2_sq = np_mass_sq(pA2)\n",
    "\n",
    "        elif self.learn_mode_sq == 'sqrt':\n",
    "            mC1_sq = Mass(qc1)\n",
    "            mC2_sq = Mass(qc2)\n",
    "            mB1_sq = Mass(pB1)\n",
    "            mB2_sq = Mass(pB2)\n",
    "            mA1_sq = Mass(pA1)\n",
    "            mA2_sq = Mass(pA2)\n",
    "        \n",
    "        mCs = torch.ones_like(mC1_sq)\n",
    "        mBs = torch.ones_like(mB1_sq)\n",
    "        mAs = torch.ones_like(mA1_sq)\n",
    "        mCs[y_0_ics[:,0]] *= self.m1_C\n",
    "        mBs[y_0_ics[:,0]] *= self.m1_B\n",
    "        mAs[y_0_ics[:,0]] *= self.m1_A\n",
    "        mCs[y_1_ics[:,0]] *= self.m2_C\n",
    "        mBs[y_1_ics[:,0]] *= self.m2_B\n",
    "        mAs[y_1_ics[:,0]] *= self.m2_A\n",
    "\n",
    "        loss_C = torch.abs(mC1_sq - mC2_sq) + torch.abs(mC1_sq - mCs) + torch.abs(mC2_sq - mCs)\n",
    "        loss_B = torch.abs(mB1_sq - mB2_sq) + torch.abs(mB1_sq - mBs) + torch.abs(mB2_sq - mBs)\n",
    "        loss_A = torch.abs(mA1_sq - mA2_sq) + torch.abs(mA1_sq - mAs) + torch.abs(mA2_sq - mAs)\n",
    "        \n",
    "        loss_pT = pT[:,0]**2 + pT[:,1]**2            \n",
    "        \n",
    "        loss_C = loss_C \n",
    "        loss_B = loss_B \n",
    "        loss_A = loss_A \n",
    "        \n",
    "        if self.learn_mode == 'pt_mc':\n",
    "            loss = (loss_A + loss_B + loss_C).mean() + loss_pT.mean()\n",
    "        elif self.learn_mode == 'pt':\n",
    "            loss = (loss_A + loss_B).mean() + loss_pT.mean()\n",
    "        elif self.learn_mode == 'mc':\n",
    "            loss = (loss_A + loss_B + loss_C).mean() \n",
    "        elif self.learn_mode == None:        \n",
    "            loss = (loss_A + loss_B).mean() \n",
    "\n",
    "        loss_C = torch.abs(mC1_sq - mC2_sq) + torch.abs(mC1_sq - mCs) + torch.abs(mC2_sq - mCs)\n",
    "        loss_B = torch.abs(mB1_sq - mB2_sq) + torch.abs(mB1_sq - mBs) + torch.abs(mB2_sq - mBs)\n",
    "        loss_A = torch.abs(mA1_sq - mA2_sq) + torch.abs(mA1_sq - mAs) + torch.abs(mA2_sq - mAs)\n",
    "        \n",
    "        loss_pT = pT[:,0]**2 + pT[:,1]**2            \n",
    "        \n",
    "        loss_C = loss_C \n",
    "        loss_B = loss_B \n",
    "        loss_A = loss_A \n",
    "        \n",
    "        if self.learn_mode == 'pt_mc':\n",
    "            loss = (loss_A + loss_B + loss_C).mean() + loss_pT.mean()\n",
    "        elif self.learn_mode == 'pt':\n",
    "            loss = (loss_A + loss_B).mean() + loss_pT.mean()\n",
    "        elif self.learn_mode == 'mc':\n",
    "            loss = (loss_A + loss_B + loss_C).mean() \n",
    "        elif self.learn_mode == None:        \n",
    "            loss = (loss_A + loss_B).mean() \n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('loss_A', loss_A)\n",
    "        self.log('loss_B', loss_B)\n",
    "        self.log('loss_C', loss_C)\n",
    "        self.log('loss_pT', loss_pT)\n",
    "        \n",
    "        m1_A1 = mA1_sq[y_0_ics[:,0]].mean()\n",
    "        m1_A2 = mA2_sq[y_0_ics[:,0]].mean()\n",
    "        m1_B1 = mB1_sq[y_0_ics[:,0]].mean()\n",
    "        m1_B2 = mB2_sq[y_0_ics[:,0]].mean()\n",
    "        m1_C1 = mC1_sq[y_0_ics[:,0]].mean()\n",
    "        m1_C2 = mC2_sq[y_0_ics[:,0]].mean()\n",
    "        m2_A1 = mA1_sq[y_1_ics[:,0]].mean()\n",
    "        m2_A2 = mA2_sq[y_1_ics[:,0]].mean()\n",
    "        m2_B1 = mB1_sq[y_1_ics[:,0]].mean()\n",
    "        m2_B2 = mB2_sq[y_1_ics[:,0]].mean()\n",
    "        m2_C1 = mC1_sq[y_1_ics[:,0]].mean()\n",
    "        m2_C2 = mC2_sq[y_1_ics[:,0]].mean()\n",
    "        \n",
    "        self.log('m1_A', self.m1_A)\n",
    "        self.log('m1_B', self.m1_B)\n",
    "        self.log('m1_C', self.m1_C)\n",
    "        self.log('m1_A1', m1_A1)\n",
    "        self.log('m1_A2', m1_A2)\n",
    "        self.log('m1_B1', m1_B1)\n",
    "        self.log('m1_B2', m1_B2)\n",
    "        self.log('m1_C1', m1_C1)\n",
    "        self.log('m1_C2', m1_C2)\n",
    "\n",
    "        self.log('m2_A', self.m2_A)\n",
    "        self.log('m2_B', self.m2_B)\n",
    "        self.log('m2_C', self.m2_C)\n",
    "        self.log('m2_A1', m2_A1)\n",
    "        self.log('m2_A2', m2_A2)\n",
    "        self.log('m2_B1', m2_B1)\n",
    "        self.log('m2_B2', m2_B2)\n",
    "        self.log('m2_C1', m2_C1)\n",
    "        self.log('m2_C2', m2_C2)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "#             betas=(0.99, 0.9999),\n",
    "#             weight_decay=0.1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": OneCycleLR(\n",
    "                    optimizer, \n",
    "                    max_lr=self.max_lr,\n",
    "                    steps_per_epoch=len(self.ds_train) // self.batch_size + 1,\n",
    "                    epochs = self.epochs,\n",
    "                ),\n",
    "                \"interval\": \"step\",\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"strict\": True,\n",
    "            }\n",
    "        } \n",
    "\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.ds = ds\n",
    "        self.N = len(self.ds)\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        N_train = self.N // 10 * 7\n",
    "        N_val = self.N - N_train\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.ds_train, self.ds_val = random_split(self.ds, [N_train, N_val])\n",
    "        if stage == \"test\" or stage is None:\n",
    "            _, self.ds_test = random_split(self.ds, [N_train, N_val])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train, batch_size=self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds_val, batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.ds_test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "407ec567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 8407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8407"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(8407)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a33fa2",
   "metadata": {},
   "source": [
    "## Hyper-parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e33fde95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = 700/scaler \n",
    "\n",
    "hparams = {\n",
    "    \"hidden_layer\": 256,\n",
    "    \"hidden_depth\": 5,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"m1_C_init\": mc,\n",
    "    \"m1_B_add\": 0.3,\n",
    "    \"m1_A_add\": 0.3,\n",
    "    \"m2_C_init\": mc,\n",
    "    \"m2_B_add\": 0.3,\n",
    "    \"m2_A_add\": 0.3,\n",
    "    \"max_lr\": 1e-4,\n",
    "    \"epochs\": 300,\n",
    "    \"gamma\": 0.9,\n",
    "    \"learn_mode\": None, # 'pt_mc', 'mc', 'pt', None\n",
    "    \"learn_mode_sq\":'sqrt', # 'sq' , 'sqrt' // Note: For 'sqrt', ONLY 'pt' and None are available for the physical reason.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16bffb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maxect\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/xteca/Documents/Project/Research/NewMissing/wandb/run-20221008_153342-fe0159q9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/axect/Auxiliary_Mass/runs/fe0159q9\" target=\"_blank\">vital-bird-189</a></strong> to <a href=\"https://wandb.ai/axect/Auxiliary_Mass\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = ToyNet(\n",
    "    hparams=hparams\n",
    ")\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='Auxiliary_Mass'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=hparams[\"epochs\"],\n",
    "    gpus=AVAIL_GPUS,\n",
    "    enable_progress_bar=False,\n",
    "    callbacks=[\n",
    "#         EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\"),\n",
    "        LearningRateMonitor(logging_interval=\"step\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b8124",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f35a68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | net  | Sequential | 337 K \n",
      "------------------------------------\n",
      "337 K     Trainable params\n",
      "0         Non-trainable params\n",
      "337 K     Total params\n",
      "1.350     Total estimated model params size (MB)\n",
      "/home/xteca/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/xteca/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "711af28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/xteca/Documents/Project/Research/NewMissing/wandb/run-20221008_153342-fe0159q9/files/Aux_Toy.pth']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_checkpoint('Aux_Toy.pth')\n",
    "wandb.save('Aux_Toy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33add5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9846aed6750b4cc6ba39c39aaaf203a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.910 MB of 3.910 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss_A</td><td>█▇▆▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_B</td><td>█▇▇▆▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_C</td><td>██▇▇▆▆▆▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁</td></tr><tr><td>loss_pT</td><td>█▇▇▇▆▆▆▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr-Adam</td><td>▁▂▂▃▃▄▅▆▇▇███████▇▇▇▇▆▆▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>m1_A</td><td>█████▇▇▆▆▅▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>m1_A1</td><td>▅▆▇████▆▆▆▅▄▃▃▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>m1_A2</td><td>▅▆▇███▇▇▇▅▅▄▄▂▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>m1_B</td><td>████▇▇▆▅▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>m1_B1</td><td>▅▆▇████▆▆▅▅▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>m1_B2</td><td>▆▆▇███▇▇▆▅▄▄▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>m1_C</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>m1_C1</td><td>▁██▁▁▁█▁▁▁▁█▁█▁██▁█▁█▁▁███▁▁███▁█▁█▁██▁▁</td></tr><tr><td>m1_C2</td><td>█▁▁███▁██▁▁█▁▁▁▁██▁▁█▁█▁█▁▁█▁█▁█▁▁▁▁▁▁▁▁</td></tr><tr><td>m2_A</td><td>███▇▇▆▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>m2_A1</td><td>█▇▇▆▆▆▅▄▄▃▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>m2_A2</td><td>█▇▆▆▆▆▅▅▄▃▃▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>m2_B</td><td>███▇▇▆▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂</td></tr><tr><td>m2_B1</td><td>█▇▆▆▆▆▅▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>m2_B2</td><td>█▇▆▆▆▆▅▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>m2_C</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>m2_C1</td><td>▁██▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁█▁▁▁▁▁█▁█▁██▁▁▁▁▁▁█▁█</td></tr><tr><td>m2_C2</td><td>█▁██▁▁▁████▁███▁██▁████▁█▁█▁▁▁▁▁▁▁█▁█▁█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>299</td></tr><tr><td>loss_A</td><td>0.15263</td></tr><tr><td>loss_B</td><td>0.09612</td></tr><tr><td>loss_C</td><td>0.0</td></tr><tr><td>loss_pT</td><td>0.0</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>m1_A</td><td>0.99968</td></tr><tr><td>m1_A1</td><td>1.00925</td></tr><tr><td>m1_A2</td><td>1.00777</td></tr><tr><td>m1_B</td><td>0.80059</td></tr><tr><td>m1_B1</td><td>0.80398</td></tr><tr><td>m1_B2</td><td>0.80348</td></tr><tr><td>m1_C</td><td>0.7</td></tr><tr><td>m1_C1</td><td>0.7</td></tr><tr><td>m1_C2</td><td>0.7</td></tr><tr><td>m2_A</td><td>1.18059</td></tr><tr><td>m2_A1</td><td>1.16147</td></tr><tr><td>m2_A2</td><td>1.16285</td></tr><tr><td>m2_B</td><td>0.89195</td></tr><tr><td>m2_B1</td><td>0.88122</td></tr><tr><td>m2_B2</td><td>0.8831</td></tr><tr><td>m2_C</td><td>0.7</td></tr><tr><td>m2_C1</td><td>0.7</td></tr><tr><td>m2_C2</td><td>0.7</td></tr><tr><td>trainer/global_step</td><td>16499</td></tr><tr><td>val_loss</td><td>0.24875</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vital-bird-189</strong>: <a href=\"https://wandb.ai/axect/Auxiliary_Mass/runs/fe0159q9\" target=\"_blank\">https://wandb.ai/axect/Auxiliary_Mass/runs/fe0159q9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221008_153342-fe0159q9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
