{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deelema](Deelema.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Uniform\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR, OneCycleLR\n",
    "from torch_poly_lr_decay import PolynomialLRDecay\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import wandb\n",
    "\n",
    "PATH_DATASETS = \".\"\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 1024 if AVAIL_GPUS else 64\n",
    "# BATCH_SIZE=1\n",
    "\n",
    "plt.rc('font', size=20)        # 기본 폰트 크기\n",
    "plt.rc('axes', labelsize=20)   # x,y축 label 폰트 크기\n",
    "plt.rc('xtick', labelsize=20)  # x축 눈금 폰트 크기 \n",
    "plt.rc('ytick', labelsize=20)  # y축 눈금 폰트 크기\n",
    "plt.rc('legend', fontsize=20)  # 범례 폰트 크기\n",
    "plt.rc('figure', titlesize=20) # figure title 폰트 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(8407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyData(Dataset):\n",
    "    def __init__(self, p_A, p_a, p_B, p_b, q_C, q_c):\n",
    "        self.X = torch.column_stack([p_A, p_a, p_B, p_b])\n",
    "        self.q_C = q_C\n",
    "        self.q_c = q_c\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.q_C[idx], self.q_c[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = 'toy'\n",
    "np_data = np.load(process+'_array_preproc.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa1 = np_data['b1']\n",
    "pa2 = np_data['b2']\n",
    "pb1 = np_data['l2']\n",
    "pb2 = np_data['l1']\n",
    "qc1 = np_data['nu1']\n",
    "qc2 = np_data['nu2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_mass_sq(p):\n",
    "    return p[:,0]**2 - p[:,1]**2 - p[:,2]**2 - p[:,3]**2\n",
    "\n",
    "g = np.array([1,-1,-1,-1])\n",
    "def Mass(p, ax=1):\n",
    "    return torch.sqrt(p[:,0]**2 - p[:,1]**2 - p[:,2]**2 - p[:,3]**2)\n",
    "\n",
    "def npMass(p, ax=1):\n",
    "    return np.sqrt(p[:,0]**2 - p[:,1]**2 - p[:,2]**2 - p[:,3]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa1.shape, pb2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Into PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_momenta = [pa1, pa2, pb1, pb2, qc1, qc2]\n",
    "\n",
    "X = np.concatenate((pa1,pa2,pb1,pb2,qc1,qc2), axis=1)\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = 1000\n",
    "\n",
    "torch_momenta_train = list(map(lambda x: torch.tensor(x/scaler, dtype=torch.float32), np.array_split(X_train, 6, axis=1)))\n",
    "torch_momenta_test  = list(map(lambda x: torch.tensor(x/scaler, dtype=torch.float32), np.array_split(X_test, 6, axis=1)))\n",
    "# torch_momenta = list(map(lambda x: torch.tensor(x / 1000, dtype=torch.float32), np_momenta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('test_sample_toy_new.txt', X_test[:,:16], fmt = '%5f', delimiter = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds       = ToyData(*torch_momenta_train)\n",
    "ds_test  = ToyData(*torch_momenta_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyNet(pl.LightningModule):\n",
    "    def __init__(self, hparams=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        hidden_layer       = hparams[\"hidden_layer\"]\n",
    "        hidden_depth       = hparams[\"hidden_depth\"]\n",
    "        learning_rate      = hparams[\"learning_rate\"]\n",
    "        batch_size         = hparams[\"batch_size\"]\n",
    "        \n",
    "        self.hidden_layer  = hidden_layer\n",
    "        self.hidden_depth  = hidden_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size    = batch_size\n",
    "        self.epochs        = hparams[\"epochs\"]\n",
    "        self.gamma        = hparams[\"gamma\"]\n",
    "        self.max_lr        = hparams['max_lr']\n",
    "        self.learn_mode    = hparams['learn_mode'] # for pT mC loss on off\n",
    "        self.learn_mode_sq = hparams['learn_mode_sq'] # sq or sqrt\n",
    "        \n",
    "        m_C = torch.tensor(hparams[\"m_C_init\"])\n",
    "        m_B = m_C + torch.tensor(hparams[\"m_B_add\"])\n",
    "        m_A = m_B + torch.tensor(hparams[\"m_A_add\"])\n",
    "\n",
    "        \n",
    "        if self.learn_mode_sq == 'sq':\n",
    "            m_C = m_C ** 2\n",
    "            m_B = m_B ** 2\n",
    "            m_A = m_A ** 2\n",
    "        elif self.learn_mode_sq == 'sqrt':        \n",
    "            m_C = m_C \n",
    "            m_B = m_B \n",
    "            m_A = m_A \n",
    "        \n",
    "#         self.m_C = nn.Parameter(m_C, requires_grad=True)\n",
    "        self.m_C = m_C\n",
    "        self.m_B = nn.Parameter(m_B, requires_grad=True)\n",
    "        self.m_A = nn.Parameter(m_A, requires_grad=True)\n",
    "        \n",
    "        layers = [nn.Linear(16, hidden_layer), nn.ReLU(inplace=True), nn.BatchNorm1d(hidden_layer)]\n",
    "        for i in range(hidden_depth):\n",
    "            layers.extend([\n",
    "                nn.Linear(hidden_layer, hidden_layer),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(hidden_layer)\n",
    "        ])\n",
    "            \n",
    "\n",
    "        if self.learn_mode == 'pt_mc':\n",
    "            layers.append(nn.Linear(hidden_layer, 8))\n",
    "        elif self.learn_mode in ['pt', 'mc']:\n",
    "            layers.append(nn.Linear(hidden_layer, 6))\n",
    "        elif self.learn_mode == None:\n",
    "            layers.append(nn.Linear(hidden_layer, 4))\n",
    "                \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        self.save_hyperparameters(hparams)\n",
    "        \n",
    "        self.ds = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "            \n",
    "            \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _, _ = batch\n",
    "        pa1 = x[:,0:4]\n",
    "        pa2 = x[:,4:8]\n",
    "        pb1 = x[:,8:12]\n",
    "        pb2 = x[:,12:16]\n",
    "        \n",
    "        q = self(x)\n",
    "        if self.learn_mode == 'pt_mc':\n",
    "            qc1 = q[:,0:4]\n",
    "            qc2 = q[:,4:8]\n",
    "        elif self.learn_mode == 'pt':\n",
    "            qx1 = q[:,0:1] \n",
    "            qy1 = q[:,1:2]\n",
    "            qx2 = q[:,2:3] \n",
    "            qy2 = q[:,3:4]\n",
    "            qz1 = q[:,4:5]   \n",
    "            qz2 = q[:,5:6]          \n",
    "            \n",
    "            Eq1 = torch.sqrt(self.m_C**2 + qx1**2 + qy1**2 + qz1**2)\n",
    "            Eq2 = torch.sqrt(self.m_C**2 + qx2**2 + qy2**2 + qz2**2)    \n",
    "            \n",
    "            qc1  = torch.cat([Eq1,qx1,qy1,qz1], 1)\n",
    "            qc2  = torch.cat([Eq2,qx2,qy2,qz2], 1) \n",
    "            \n",
    "        elif self.learn_mode == 'mc':\n",
    "            qx1 = q[:,0:1] \n",
    "            qy2 = q[:,1:2]\n",
    "            qz1 = q[:,2:3]   \n",
    "            qz2 = q[:,3:4]\n",
    "            Eq1 = q[:,4:5]\n",
    "            Eq2 = q[:,5:6]            \n",
    "\n",
    "            pTx = x[:,1:2]+x[:,5:6]+x[:,9:10]+x[:,13:14]\n",
    "            pTy = x[:,2:3]+x[:,6:7]+x[:,10:11]+x[:,14:15]\n",
    "\n",
    "            qx2 = -pTx-qx1\n",
    "            qy1 = -pTy-qy2\n",
    "\n",
    "            qc1  = torch.cat([Eq1,qx1,qy1,qz1], 1)\n",
    "            qc2  = torch.cat([Eq2,qx2,qy2,qz2], 1)    \n",
    "            \n",
    "        elif self.learn_mode == None:\n",
    "            qx1 = q[:,0:1] \n",
    "            qy2 = q[:,1:2]\n",
    "            qz1 = q[:,2:3]   \n",
    "            qz2 = q[:,3:4]\n",
    "\n",
    "            pTx = x[:,1:2]+x[:,5:6]+x[:,9:10]+x[:,13:14]\n",
    "            pTy = x[:,2:3]+x[:,6:7]+x[:,10:11]+x[:,14:15]\n",
    "\n",
    "            qx2 = -pTx-qx1\n",
    "            qy1 = -pTy-qy2\n",
    "\n",
    "            Eq1 = torch.sqrt(self.m_C**2 + qx1**2 + qy1**2 + qz1**2)\n",
    "            Eq2 = torch.sqrt(self.m_C**2 + qx2**2 + qy2**2 + qz2**2)\n",
    "\n",
    "            qc1  = torch.cat([Eq1,qx1,qy1,qz1], 1)\n",
    "            qc2  = torch.cat([Eq2,qx2,qy2,qz2], 1)        \n",
    "        \n",
    "        pB1 = pb1 + qc1\n",
    "        pB2 = pb2 + qc2\n",
    "        pA1 = pa1 + pB1\n",
    "        pA2 = pa2 + pB2\n",
    "        pT = (pA1 + pA2)[:,1:3]\n",
    "\n",
    "        if self.learn_mode_sq == 'sq':\n",
    "            mC1_sq = np_mass_sq(qc1)\n",
    "            mC2_sq = np_mass_sq(qc2)\n",
    "            mB1_sq = np_mass_sq(pB1)\n",
    "            mB2_sq = np_mass_sq(pB2)\n",
    "            mA1_sq = np_mass_sq(pA1)\n",
    "            mA2_sq = np_mass_sq(pA2)\n",
    "\n",
    "        elif self.learn_mode_sq == 'sqrt':\n",
    "            mC1_sq = Mass(qc1)\n",
    "            mC2_sq = Mass(qc2)\n",
    "            mB1_sq = Mass(pB1)\n",
    "            mB2_sq = Mass(pB2)\n",
    "            mA1_sq = Mass(pA1)\n",
    "            mA2_sq = Mass(pA2)\n",
    "\n",
    "        mCs = self.m_C * torch.ones_like(mC1_sq)\n",
    "        mBs = self.m_B * torch.ones_like(mB1_sq)\n",
    "        mAs = self.m_A * torch.ones_like(mA1_sq)\n",
    "        \n",
    "  \n",
    "\n",
    "        loss_C = torch.abs(mC1_sq - mC2_sq) + torch.abs(mC1_sq - mCs) + torch.abs(mC2_sq - mCs)\n",
    "        loss_B = torch.abs(mB1_sq - mB2_sq) + torch.abs(mB1_sq - mBs) + torch.abs(mB2_sq - mBs)\n",
    "        loss_A = torch.abs(mA1_sq - mA2_sq) + torch.abs(mA1_sq - mAs) + torch.abs(mA2_sq - mAs)\n",
    "        \n",
    "        loss_pT = pT[:,0]**2 + pT[:,1]**2            \n",
    "        \n",
    "        loss_C = loss_C \n",
    "        loss_B = loss_B \n",
    "        loss_A = loss_A \n",
    "        \n",
    "        \n",
    "        if self.learn_mode == 'pt_mc':\n",
    "            loss = (loss_A + loss_B + loss_C).mean() + loss_pT.mean()\n",
    "        elif self.learn_mode == 'pt':\n",
    "            loss = (loss_A + loss_B).mean() + loss_pT.mean()\n",
    "        elif self.learn_mode == 'mc':\n",
    "            loss = (loss_A + loss_B + loss_C).mean() \n",
    "        elif self.learn_mode == None:        \n",
    "            loss = (loss_A + loss_B).mean() \n",
    "                \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _, _ = batch\n",
    "        pa1 = x[:,0:4]\n",
    "        pa2 = x[:,4:8]\n",
    "        pb1 = x[:,8:12]\n",
    "        pb2 = x[:,12:16]\n",
    "        \n",
    "        q = self(x)\n",
    "        if self.learn_mode == 'pt_mc':\n",
    "            qc1 = q[:,0:4]\n",
    "            qc2 = q[:,4:8]\n",
    "        elif self.learn_mode == 'pt':\n",
    "            qx1 = q[:,0:1] \n",
    "            qy1 = q[:,1:2]\n",
    "            qx2 = q[:,2:3] \n",
    "            qy2 = q[:,3:4]\n",
    "            qz1 = q[:,4:5]   \n",
    "            qz2 = q[:,5:6]          \n",
    "            \n",
    "            Eq1 = torch.sqrt(self.m_C**2 + qx1**2 + qy1**2 + qz1**2)\n",
    "            Eq2 = torch.sqrt(self.m_C**2 + qx2**2 + qy2**2 + qz2**2)    \n",
    "            \n",
    "            qc1  = torch.cat([Eq1,qx1,qy1,qz1], 1)\n",
    "            qc2  = torch.cat([Eq2,qx2,qy2,qz2], 1) \n",
    "            \n",
    "        elif self.learn_mode == 'mc':\n",
    "            qx1 = q[:,0:1] \n",
    "            qy2 = q[:,1:2]\n",
    "            qz1 = q[:,2:3]   \n",
    "            qz2 = q[:,3:4]\n",
    "            Eq1 = q[:,4:5]\n",
    "            Eq2 = q[:,5:6]            \n",
    "\n",
    "            pTx = x[:,1:2]+x[:,5:6]+x[:,9:10]+x[:,13:14]\n",
    "            pTy = x[:,2:3]+x[:,6:7]+x[:,10:11]+x[:,14:15]\n",
    "\n",
    "            qx2 = -pTx-qx1\n",
    "            qy1 = -pTy-qy2\n",
    "\n",
    "            qc1  = torch.cat([Eq1,qx1,qy1,qz1], 1)\n",
    "            qc2  = torch.cat([Eq2,qx2,qy2,qz2], 1)    \n",
    "            \n",
    "        elif self.learn_mode == None:\n",
    "            qx1 = q[:,0:1] \n",
    "            qy2 = q[:,1:2]\n",
    "            qz1 = q[:,2:3]   \n",
    "            qz2 = q[:,3:4]\n",
    "\n",
    "            pTx = x[:,1:2]+x[:,5:6]+x[:,9:10]+x[:,13:14]\n",
    "            pTy = x[:,2:3]+x[:,6:7]+x[:,10:11]+x[:,14:15]\n",
    "\n",
    "            qx2 = -pTx-qx1\n",
    "            qy1 = -pTy-qy2\n",
    "\n",
    "            Eq1 = torch.sqrt(self.m_C**2 + qx1**2 + qy1**2 + qz1**2)\n",
    "            Eq2 = torch.sqrt(self.m_C**2 + qx2**2 + qy2**2 + qz2**2)\n",
    "\n",
    "            qc1  = torch.cat([Eq1,qx1,qy1,qz1], 1)\n",
    "            qc2  = torch.cat([Eq2,qx2,qy2,qz2], 1)        \n",
    "        \n",
    "        pB1 = pb1 + qc1\n",
    "        pB2 = pb2 + qc2\n",
    "        pA1 = pa1 + pB1\n",
    "        pA2 = pa2 + pB2\n",
    "        pT = (pA1 + pA2)[:,1:3]\n",
    "\n",
    "        if self.learn_mode_sq == 'sq':\n",
    "            mC1_sq = np_mass_sq(qc1)\n",
    "            mC2_sq = np_mass_sq(qc2)\n",
    "            mB1_sq = np_mass_sq(pB1)\n",
    "            mB2_sq = np_mass_sq(pB2)\n",
    "            mA1_sq = np_mass_sq(pA1)\n",
    "            mA2_sq = np_mass_sq(pA2)\n",
    "\n",
    "        elif self.learn_mode_sq == 'sqrt':\n",
    "            mC1_sq = Mass(qc1)\n",
    "            mC2_sq = Mass(qc2)\n",
    "            mB1_sq = Mass(pB1)\n",
    "            mB2_sq = Mass(pB2)\n",
    "            mA1_sq = Mass(pA1)\n",
    "            mA2_sq = Mass(pA2)\n",
    "\n",
    "        mCs = self.m_C * torch.ones_like(mC1_sq)\n",
    "        mBs = self.m_B * torch.ones_like(mB1_sq)\n",
    "        mAs = self.m_A * torch.ones_like(mA1_sq)\n",
    "        \n",
    "  \n",
    "\n",
    "        loss_C = torch.abs(mC1_sq - mC2_sq) + torch.abs(mC1_sq - mCs) + torch.abs(mC2_sq - mCs)\n",
    "        loss_B = torch.abs(mB1_sq - mB2_sq) + torch.abs(mB1_sq - mBs) + torch.abs(mB2_sq - mBs)\n",
    "        loss_A = torch.abs(mA1_sq - mA2_sq) + torch.abs(mA1_sq - mAs) + torch.abs(mA2_sq - mAs)\n",
    "        \n",
    "        loss_pT = pT[:,0]**2 + pT[:,1]**2            \n",
    "        \n",
    "        loss_C = loss_C \n",
    "        loss_B = loss_B \n",
    "        loss_A = loss_A \n",
    "        \n",
    "        \n",
    "        if self.learn_mode == 'pt_mc':\n",
    "            loss = (loss_A + loss_B + loss_C).mean() + loss_pT.mean()\n",
    "        elif self.learn_mode == 'pt':\n",
    "            loss = (loss_A + loss_B).mean() + loss_pT.mean()\n",
    "        elif self.learn_mode == 'mc':\n",
    "            loss = (loss_A + loss_B + loss_C).mean() \n",
    "        elif self.learn_mode == None:        \n",
    "            loss = (loss_A + loss_B).mean() \n",
    "            \n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('loss_A', loss_A)\n",
    "        self.log('loss_B', loss_B)\n",
    "        self.log('loss_C', loss_C)\n",
    "        self.log('loss_pT', loss_pT)\n",
    "        self.log('m_A', self.m_A)\n",
    "        self.log('m_B', self.m_B)\n",
    "        self.log('m_C', self.m_C)\n",
    "        self.log('m_A1', mA1_sq)\n",
    "        self.log('m_A2', mA2_sq)\n",
    "        self.log('m_B1', mB1_sq)\n",
    "        self.log('m_B2', mB2_sq)\n",
    "        self.log('m_C1', mC1_sq)\n",
    "        self.log('m_C2', mC2_sq)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            betas=(0.99, 0.9999),\n",
    "            weight_decay=0.1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": OneCycleLR(\n",
    "                    optimizer, \n",
    "                    max_lr=self.max_lr,\n",
    "                    steps_per_epoch=len(self.ds_train) // self.batch_size + 1,\n",
    "                    epochs = self.epochs,\n",
    "                ),\n",
    "                \"interval\": \"step\",\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"strict\": True,\n",
    "            }\n",
    "        } \n",
    "\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.ds = ToyData(*torch_momenta_train)\n",
    "        self.N = len(self.ds)\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        N_train = self.N // 10 * 7\n",
    "        N_val = self.N - N_train\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.ds_train, self.ds_val = random_split(self.ds, [N_train, N_val])\n",
    "        if stage == \"test\" or stage is None:\n",
    "            _, self.ds_test = random_split(self.ds, [N_train, N_val])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train, batch_size=self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds_val, batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.ds_test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = 700/scaler \n",
    "\n",
    "hparams = {\n",
    "    \"hidden_layer\": 256,\n",
    "    \"hidden_depth\": 5,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"m_C_init\": mc,\n",
    "    \"m_B_add\": 0.3,\n",
    "    \"m_A_add\": 0.3,\n",
    "    \"max_lr\": 1e-4,\n",
    "    \"epochs\": 50,\n",
    "    \"gamma\": 0.9,\n",
    "    \"learn_mode\": None, # 'pt_mc', 'mc', 'pt', None\n",
    "    \"learn_mode_sq\":'sqrt', # 'sq' , 'sqrt' // Note: For 'sqrt', ONLY 'pt' and None are available for the physical reason.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyNet(\n",
    "    hparams=hparams\n",
    ")\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='Auxiliary_Mass_Exp_boostaug'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=hparams[\"epochs\"],\n",
    "    gpus=AVAIL_GPUS,\n",
    "    enable_progress_bar=False,\n",
    "    callbacks=[\n",
    "#         EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\"),\n",
    "        LearningRateMonitor(logging_interval=\"step\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint('Auxiliary.pth')\n",
    "wandb.save('Auxiliary.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
